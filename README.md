# 从零开始构建神经网络图像分类器
本项目实现了一个包含全连接层与批量归一化层的神经网络分类器，在CIFAR-10数据集上采用小批次训练，并采用SGD优化，训练模型实现图像分类。

# 数据处理
首先将原始图像数据进行标准化处理，并将图像标签进行独热编码，将训练集打乱顺序后，按照9:1的比例划分为训练集和验证集。

# 模型构建
这里设计一个三层神经网络，包含两个隐藏层，可进行前向传播，以及通过反向传播计算给定损失的梯度。该神经网络包含3个全连接层（FCN），支持自定义激活函数(ReLU/LeakyReLU/Sigmoid/Tanh)，支持自定义学习率、正则化强度、隐藏层大小等超参数。并在FCN之间添加了批量归一化层（BN），最终通过softmax函数转化为概率分布输出。

网络结构如下：输入->FCN1->BN1->FCN2->BN2->FCN3->softmax->输出。

其中，FCN的权重矩阵和偏置向量，以及BN的缩放因子和平移因子，均为可学习参数，通过SGD优化器进行参数更新优化。

# 模型训练
训练进行多轮迭代，每轮迭代采用小批次训练，每次使用一个批次（大小可自定义）的数据。输入数据为3072维的数据，进行前向传播，经过相应FCN层和BN层计算，输出得到10维的概率分布值。根据损失，计算含L2正则化项的交叉熵损失值，通过梯度计算反向传播，应用SGD优化进行参数更新，更新的参数包含FCN层的权重矩阵、偏置向量以及BN层的缩放、平移因子。

在迭代的过程中，程序会记录训练集上的交叉熵损失值，以及验证集上的损失值和准确率。训练进行多轮迭代，学习率逐渐衰减（衰减策略可选），并在验证集上进行模型评估，保存当前最优模型。

训练过程添加了早停机制，当验证集上的准确率连续若干轮没有提高时，停止训练。

在该网络模型中，存在以下超参数：隐藏层大小、学习率、正则化强度、学习率衰减率、批次大小、最大迭代次数、激活函数。

# 代码使用
运行程序时，主体函数已经放在主函数中，需要指定数据集地址。

通过ThreeLayerNet()函数的参数指定网络的隐藏层大小以及激活函数，构建初始模型。

通过train()函数指定学习率、正则化强度、学习率衰减率、批次大小、迭代次数并进行模型训练，其中后三者有默认值，train()运行输出模型，同时保存模型权重以及损失曲线、准确率曲线的图像（需要修改train()函数中的输出地址）。

test()函数会在测试集上对模型进行测试并返回准确率。

parameter_search()函数可进行参数查找，参数搜索空间可在函数定义中修改，函数运行返回最优参数组合。

代码中将全连接层和批量归一化层定义为类并分别给出了初始化、前向传播、反向传播以及参数更新的代码，这样使得神经网络模型的构建更加灵活方便。
代码文件中包含一些被注释掉的代码，这些是一些可选项，可以根据需要添加或运行。


