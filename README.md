从零开始构建三层神经网络分类器，实现图像分类


CIFAR-10数据集包含60000张32x32像素的彩色图像，分为10个类别，每个类别包含6000张图像。
数据集被分为50000张训练图像和10000张测试图像，训练/测试图像比例为5:1。
\subsection{数据预处理}
实验将原始图像数据进行了标准化处理，并将图像标签进行了独热编码，将训练集打乱顺序后，
按照9:1的比例划分为训练集和验证集。

\section{模型构建}
\subsection{模型架构}
设计一个三层神经网络，包含两个隐藏层，可进行前向传播，以及通过反向传播计算给定损失的梯度。
该神经网络包含3个全连接层（FCN），支持自定义激活函数(ReLU/LeakyReLU/Sigmoid/Tanh)，
支持自定义学习率、正则化强度、隐藏层大小等超参数。
并在FCN之间添加了批量归一化层（BN），最终通过softmax函数转化为概率分布输出。
网络结构如下：
输入-$>$FCN1-$>$BN1-$>$FCN2-$>$BN2-$>$FCN3-$>$softmax-$>$输出



其中，FCN的权重矩阵和偏置向量，以及BN的缩放因子和平移因子，均为可学习参数，
通过反向传播计算给定损失的梯度，并使用SGD优化器进行参数更新，随着迭代次数的增加，
学习率逐渐下降。

\section{模型训练}
\subsection{训练概述}
在训练过程中，采用小批次训练，每次训练使用一个批次（大小可自定义）的数据，
进行前向传播并计算损失函数，然后进行反向传播，
并使用SGD优化器进行参数更新。训练进行多轮迭代，学习率逐渐衰减（衰减策略可选）, 并在验证集上进行模型评估，保存当前最优模型。
另外训练过程添加了早停机制，当验证集上的准确率连续若干轮没有提高时，停止训练。
\subsection{模型超参数}
在该网络模型中，存在以下超参数：
\begin{tasks}[style=itemize](2)
\task 隐藏层大小（$\text{hidden\_size}$）
\task 学习率（$\text{learning\_rate}$）
\task 正则化强度（$\text{lambda}$）
\task 学习率衰减率（$\text{decay}$）
\task 批次大小（$\text{batch\_size}$）
\task 最大迭代次数（$\text{num\_epochs}$）
\task 激活函数（$\text{activation\_func}$）
\end{tasks}


\subsection{参数查找}
为了找到最佳的超参数组合，这里使用了网格搜索的方法。实验定义了一组超参数的候选值，
对每个参数组合进行训练，记录验证集上的准确率并比较。
由于时间原因，每个参数组合只训练了15轮（通过尝试可以发现此时部分模型已表现出收敛）。



\subsection{训练结果}
对于如下参数组合：隐藏层大小为$256$和$64$，学习率$0.01$，正则化强度$0.001$，学习率衰减率$0.9$，激活函数Leaky\_ReLU，批次大小$128$，最大迭代次数$100$
训练和测试的具体呈现如下：


从损失曲线和准确率曲线可以看出，模型收敛较快，验证集上的准确率在训练过程中逐渐提高后稳定在$51\%$左右。
该模型经过较少的迭代次数即可达到较好的效果，但从图3中可以看出，
模型迭代次数过多后，模型可能会出现过拟合的情况，损失值出现上升。根据多次重复实验，模型在测试集上表现较为良好，
准确率在$51\%$至$53\%$之间波动。

\subsection{可视化}
对于上述训练得到的模型，本实验对模型的网络参数进行了可视化，
下图为参数可视化结果。
\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{model_parameters.png}
\caption{参数可视化结果}
\label{fig:parameter_visualization}
\end{figure}
从图6中可以看出，模型的网络参数呈现出一定的模式，全连接层的参数基本接近于正态分布，均匀分布在0附近，
截取部分参数绘制热力图，可以看出参数分布较为均匀，该模型的网络参数基本符合预期。


\section{探究实验}
\subsection{BN层实验}
为探究BN层的作用，这里对BN层进行了实验。
实验分为两组，一组使用BN层，另一组取消BN层，两组采用相同的超参数进行训练
（超参数组合：隐藏层大小为$256$和$64$，学习率$0.01$，正则化强度$0.001$，学习率衰减率$0.9$，激活函数Leaky\_ReLU，批次大小$128$，最大迭代次数$100$）。
下图为两个模型训练和测试得到的loss曲线和accuracy曲线（图7），以及程序输出（图8）。



\caption{程序输出结果}
\label{fig:test_comparison}
\end{figure}
可以看出，使用BN层可以加速模型的收敛，训练较少轮次即可达到较好的准确率，提高模型的泛化能力，在测试集上表现良好；
取消BN层会导致模型训练速度变慢，需要更多轮次才能达到较好的准确率。
同时也发现，使用BN层会使单次迭代的时间变长，训练过久易出现过拟合问题。

\section{总结}
本实验实现了一个包含全连接层与批量归一化层的神经网络分类器，
在CIFAR-10数据集上采用小批次训练，并采用SGD优化，训练模型实现图像分类。
训练得到的模型在测试集上表现良好，准确率在$51\%$至$53\%$之间波动。
通过实验可以看出，使用BN层可以加速模型的收敛，提高模型的泛化能力，
该模型可以推广拼接到其他的分类任务中。
同时本实验构建的神经网络分类器也存在缺陷，例如参数查找耗时较长，超参数搜索不够精细，
准确率难以达到应用水平，
同时模型迭代次数较多后易出现过拟合问题，相比于卷积神经网络（CNN）存在很大差距，
该网络模型有待改善。

\end{document}
